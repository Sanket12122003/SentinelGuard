{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce GTX 1650\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:26<00:00, 6543405.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Epoch [1/100] - D Loss: 0.2679, G Loss: 4.8674\n",
      "Epoch [2/100] - D Loss: 0.0970, G Loss: 2.7648\n",
      "Epoch [3/100] - D Loss: 0.3418, G Loss: 3.5432\n",
      "Epoch [4/100] - D Loss: 0.4498, G Loss: 2.5483\n",
      "Epoch [5/100] - D Loss: 1.7149, G Loss: 4.5562\n",
      "Models saved at epoch 5\n",
      "Epoch [6/100] - D Loss: 0.8848, G Loss: 2.5202\n",
      "Epoch [7/100] - D Loss: 0.7654, G Loss: 3.0090\n",
      "Epoch [8/100] - D Loss: 1.4406, G Loss: 1.7960\n",
      "Epoch [9/100] - D Loss: 1.0109, G Loss: 1.6095\n",
      "Epoch [10/100] - D Loss: 0.6393, G Loss: 1.6690\n",
      "Models saved at epoch 10\n",
      "Epoch [11/100] - D Loss: 0.8843, G Loss: 1.7096\n",
      "Epoch [12/100] - D Loss: 0.9642, G Loss: 1.4773\n",
      "Epoch [13/100] - D Loss: 0.9911, G Loss: 2.9570\n",
      "Epoch [14/100] - D Loss: 0.9604, G Loss: 1.5489\n",
      "Epoch [15/100] - D Loss: 1.0213, G Loss: 1.1879\n",
      "Models saved at epoch 15\n",
      "Epoch [16/100] - D Loss: 1.4011, G Loss: 1.4107\n",
      "Epoch [17/100] - D Loss: 1.1780, G Loss: 1.3822\n",
      "Epoch [18/100] - D Loss: 1.2817, G Loss: 1.7663\n",
      "Epoch [19/100] - D Loss: 1.0039, G Loss: 1.2180\n",
      "Epoch [20/100] - D Loss: 1.1260, G Loss: 1.4858\n",
      "Models saved at epoch 20\n",
      "Epoch [21/100] - D Loss: 1.2304, G Loss: 1.5212\n",
      "Epoch [22/100] - D Loss: 0.9935, G Loss: 1.8438\n",
      "Epoch [23/100] - D Loss: 1.3546, G Loss: 1.0931\n",
      "Epoch [24/100] - D Loss: 1.0500, G Loss: 2.3587\n",
      "Epoch [25/100] - D Loss: 1.2147, G Loss: 1.5380\n",
      "Models saved at epoch 25\n",
      "Epoch [26/100] - D Loss: 1.0438, G Loss: 1.2677\n",
      "Epoch [27/100] - D Loss: 1.0618, G Loss: 1.2347\n",
      "Epoch [28/100] - D Loss: 1.8056, G Loss: 1.6912\n",
      "Epoch [29/100] - D Loss: 0.9400, G Loss: 1.2202\n",
      "Epoch [30/100] - D Loss: 1.4869, G Loss: 1.3606\n",
      "Models saved at epoch 30\n",
      "Epoch [31/100] - D Loss: 0.9361, G Loss: 1.7532\n",
      "Epoch [32/100] - D Loss: 1.7616, G Loss: 2.0799\n",
      "Epoch [33/100] - D Loss: 1.2174, G Loss: 1.6313\n",
      "Epoch [34/100] - D Loss: 1.0477, G Loss: 1.4040\n",
      "Epoch [35/100] - D Loss: 1.1177, G Loss: 1.4862\n",
      "Models saved at epoch 35\n",
      "Epoch [36/100] - D Loss: 1.8330, G Loss: 1.4810\n",
      "Epoch [37/100] - D Loss: 1.4261, G Loss: 1.5925\n",
      "Epoch [38/100] - D Loss: 0.9744, G Loss: 1.2733\n",
      "Epoch [39/100] - D Loss: 1.1624, G Loss: 0.9887\n",
      "Epoch [40/100] - D Loss: 0.9227, G Loss: 1.2543\n",
      "Models saved at epoch 40\n",
      "Epoch [41/100] - D Loss: 0.9006, G Loss: 1.7400\n",
      "Epoch [42/100] - D Loss: 1.2709, G Loss: 1.2568\n",
      "Epoch [43/100] - D Loss: 1.2262, G Loss: 1.0716\n",
      "Epoch [44/100] - D Loss: 1.7571, G Loss: 1.3562\n",
      "Epoch [45/100] - D Loss: 1.9824, G Loss: 3.3721\n",
      "Models saved at epoch 45\n",
      "Epoch [46/100] - D Loss: 1.5318, G Loss: 1.5144\n",
      "Epoch [47/100] - D Loss: 1.2337, G Loss: 1.5900\n",
      "Epoch [48/100] - D Loss: 1.3537, G Loss: 1.8787\n",
      "Epoch [49/100] - D Loss: 1.3449, G Loss: 2.2508\n",
      "Epoch [50/100] - D Loss: 1.1476, G Loss: 1.3652\n",
      "Models saved at epoch 50\n",
      "Epoch [51/100] - D Loss: 1.2592, G Loss: 1.3030\n",
      "Epoch [52/100] - D Loss: 2.0080, G Loss: 1.1171\n",
      "Epoch [53/100] - D Loss: 1.1825, G Loss: 1.8404\n",
      "Epoch [54/100] - D Loss: 1.1494, G Loss: 1.4646\n",
      "Epoch [55/100] - D Loss: 0.9127, G Loss: 1.7036\n",
      "Models saved at epoch 55\n",
      "Epoch [56/100] - D Loss: 1.4352, G Loss: 1.2022\n",
      "Epoch [57/100] - D Loss: 1.1712, G Loss: 1.1284\n",
      "Epoch [58/100] - D Loss: 0.9886, G Loss: 0.8668\n",
      "Epoch [59/100] - D Loss: 1.2802, G Loss: 2.0529\n",
      "Epoch [60/100] - D Loss: 1.0480, G Loss: 1.3224\n",
      "Models saved at epoch 60\n",
      "Epoch [61/100] - D Loss: 1.2843, G Loss: 2.7015\n",
      "Epoch [62/100] - D Loss: 1.5607, G Loss: 1.7024\n",
      "Epoch [63/100] - D Loss: 0.7738, G Loss: 1.4912\n",
      "Epoch [64/100] - D Loss: 1.8610, G Loss: 3.1111\n",
      "Epoch [65/100] - D Loss: 1.2571, G Loss: 1.6230\n",
      "Models saved at epoch 65\n",
      "Epoch [66/100] - D Loss: 1.0456, G Loss: 1.6350\n",
      "Epoch [67/100] - D Loss: 1.0923, G Loss: 1.9880\n",
      "Epoch [68/100] - D Loss: 1.2871, G Loss: 1.5358\n",
      "Epoch [69/100] - D Loss: 1.3410, G Loss: 1.5204\n",
      "Epoch [70/100] - D Loss: 1.3613, G Loss: 1.3666\n",
      "Models saved at epoch 70\n",
      "Epoch [71/100] - D Loss: 1.1230, G Loss: 1.6000\n",
      "Epoch [72/100] - D Loss: 0.8823, G Loss: 1.7801\n",
      "Epoch [73/100] - D Loss: 0.8443, G Loss: 1.8167\n",
      "Epoch [74/100] - D Loss: 0.6460, G Loss: 2.0192\n",
      "Epoch [75/100] - D Loss: 1.0480, G Loss: 0.9538\n",
      "Models saved at epoch 75\n",
      "Epoch [76/100] - D Loss: 1.6417, G Loss: 1.3023\n",
      "Epoch [77/100] - D Loss: 2.1917, G Loss: 2.1186\n",
      "Epoch [78/100] - D Loss: 0.9653, G Loss: 1.7535\n",
      "Epoch [79/100] - D Loss: 1.2211, G Loss: 2.4054\n",
      "Epoch [80/100] - D Loss: 1.2666, G Loss: 1.7090\n",
      "Models saved at epoch 80\n",
      "Epoch [81/100] - D Loss: 2.0094, G Loss: 1.9443\n",
      "Epoch [82/100] - D Loss: 0.7382, G Loss: 2.6011\n",
      "Epoch [83/100] - D Loss: 0.9088, G Loss: 1.7673\n",
      "Epoch [84/100] - D Loss: 1.4933, G Loss: 1.5419\n",
      "Epoch [85/100] - D Loss: 1.2081, G Loss: 1.8892\n",
      "Models saved at epoch 85\n",
      "Epoch [86/100] - D Loss: 1.5750, G Loss: 1.3983\n",
      "Epoch [87/100] - D Loss: 1.3633, G Loss: 1.3538\n",
      "Epoch [88/100] - D Loss: 1.6438, G Loss: 1.5439\n",
      "Epoch [89/100] - D Loss: 1.9067, G Loss: 2.1788\n",
      "Epoch [90/100] - D Loss: 1.5734, G Loss: 1.1224\n",
      "Models saved at epoch 90\n",
      "Epoch [91/100] - D Loss: 1.4059, G Loss: 1.7674\n",
      "Epoch [92/100] - D Loss: 0.5866, G Loss: 1.7033\n",
      "Epoch [93/100] - D Loss: 1.2653, G Loss: 1.8686\n",
      "Epoch [94/100] - D Loss: 1.2882, G Loss: 1.3810\n",
      "Epoch [95/100] - D Loss: 0.8325, G Loss: 1.8034\n",
      "Models saved at epoch 95\n",
      "Epoch [96/100] - D Loss: 1.3860, G Loss: 1.0703\n",
      "Epoch [97/100] - D Loss: 1.3528, G Loss: 1.9702\n",
      "Epoch [98/100] - D Loss: 0.9364, G Loss: 2.1630\n",
      "Epoch [99/100] - D Loss: 0.7070, G Loss: 2.9275\n",
      "Epoch [100/100] - D Loss: 2.2497, G Loss: 1.8844\n",
      "Models saved at epoch 100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Define Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 64 * 64 * 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z).view(z.size(0), 3, 64, 64)\n",
    "\n",
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 64 * 3, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "# Training function\n",
    "def train_gan(generator, discriminator, data_loader, latent_dim, epochs, device, save_path):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)  # Ensure the save directory exists\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for real_imgs, _ in data_loader:\n",
    "            real_imgs = real_imgs.to(device).view(real_imgs.size(0), -1)\n",
    "\n",
    "            # Train Discriminator\n",
    "            valid = torch.ones(real_imgs.size(0), 1).to(device)\n",
    "            fake = torch.zeros(real_imgs.size(0), 1).to(device)\n",
    "\n",
    "            optimizer_d.zero_grad()\n",
    "            real_loss = criterion(discriminator(real_imgs), valid)\n",
    "\n",
    "            z = torch.randn(real_imgs.size(0), latent_dim).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "            fake_loss = criterion(discriminator(fake_imgs.view(fake_imgs.size(0), -1)), fake)\n",
    "\n",
    "            d_loss = real_loss + fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            z = torch.randn(real_imgs.size(0), latent_dim).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "            g_loss = criterion(discriminator(fake_imgs.view(fake_imgs.size(0), -1)), valid)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "        # Save models periodically\n",
    "        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:  # Save every 5 epochs and the final epoch\n",
    "            torch.save(generator.state_dict(), os.path.join(save_path, \"generator.pth\"))\n",
    "            torch.save(discriminator.state_dict(), os.path.join(save_path, \"discriminator.pth\"))\n",
    "            print(f\"Models saved at epoch {epoch + 1}\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    latent_dim = 100\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    save_path = \"./models\"  # Directory to save models\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Models\n",
    "    generator = Generator(latent_dim).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    # Train\n",
    "    train_gan(generator, discriminator, data_loader, latent_dim, epochs, device, save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
